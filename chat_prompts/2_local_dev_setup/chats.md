## Prompt 1

Given these requirements and schema / api information, can you create a detailed list of user stories and their corresponding tasks that you will need to perform to achieve the requirements. Make the tasks granular, one task is one operation.

Requirements:
### Feature 1: Local Development & Foundation Setup

**Goal:** Establish the monorepo structure, initialize the core frameworks, and ensure the entire stack runs seamlessly on a local machine via Docker Compose.

* **Requirements:**
* Initialize a Vite + React + TypeScript frontend with Tailwind CSS and shadcn/ui.
* Initialize a FastAPI + Python backend with environment variable management.
* Set up a local PostgreSQL database instance.
* Configure Docker Compose to network the frontend, backend, and database together locally with hot-reloading enabled for development.


* **Deliverables:**
| Component | Deliverable Description |
| :--- | :--- |
| **Repository** | A standard monorepo folder structure (`/frontend`, `/backend`, `/infrastructure`). |
| **Docker** | `Dockerfile.dev` for both frontend and backend; a root `docker-compose.yml` file. |
| **Config Files** | `vite.config.ts` (proxying API requests to backend), `requirements.txt` / `pyproject.toml`, and a local `.env.example`. |

---

APIs/ Schemas:

1. Database Schemas (PostgreSQL)

To support the features—while allowing flexibility for different types of investment strategies, such as separating domestic trading from international brokerage accounts or tracking employee equity (RSUs)—we need a normalized relational structure.

#### `users`

Stores authentication and basic profile data.
| Column | Type | Constraints / Notes |
| :--- | :--- | :--- |
| `id` | UUID | Primary Key, default `uuid4()` |
| `email` | VARCHAR(255) | Unique, Not Null, Indexed |
| `password_hash` | VARCHAR(255) | Not Null (Argon2 or bcrypt) |
| `created_at` | TIMESTAMP | Default `now()` |

#### `portfolios`

Allows users to group their tracked stocks logically (e.g., "Tech Dividend Reinvestment", "Domestic Holdings").
| Column | Type | Constraints / Notes |
| :--- | :--- | :--- |
| `id` | UUID | Primary Key |
| `user_id` | UUID | Foreign Key -> `users.id`, On Delete Cascade |
| `name` | VARCHAR(100) | Not Null |
| `account_type` | VARCHAR(50) | e.g., 'DOMESTIC', 'INTERNATIONAL', 'EMPLOYEE_EQUITY' |
| `created_at` | TIMESTAMP | Default `now()` |

#### `portfolio_items`

The specific tickers associated with a portfolio.
| Column | Type | Constraints / Notes |
| :--- | :--- | :--- |
| `id` | UUID | Primary Key |
| `portfolio_id` | UUID | Foreign Key -> `portfolios.id`, On Delete Cascade |
| `ticker` | VARCHAR(20) | Not Null (e.g., 'GOOGL', 'RELIANCE.NS') |
| `added_at` | TIMESTAMP | Default `now()` |

#### `analysis_history`

Caches the Gemini analysis so users can review past AI recommendations without triggering new API calls.
| Column | Type | Constraints / Notes |
| :--- | :--- | :--- |
| `id` | UUID | Primary Key |
| `user_id` | UUID | Foreign Key -> `users.id` |
| `ticker` | VARCHAR(20) | Not Null |
| `rating` | VARCHAR(20) | 'BUY', 'HOLD', 'SELL' |
| `confidence` | INTEGER | 0 to 100 |
| `thesis` | TEXT | The detailed markdown thesis generated by Gemini |
| `created_at` | TIMESTAMP | Default `now()` |

#### `chat_sessions` & `chat_messages`

Stores the conversational memory for the WebSocket sidebar.
| Table | Column | Type | Constraints / Notes |
| :--- | :--- | :--- | :--- |
| **`chat_sessions`** | `id` | UUID | Primary Key |
| | `user_id` | UUID | Foreign Key -> `users.id` |
| | `context_ticker`| VARCHAR(20) | Nullable (If chat is tied to a specific stock) |
| **`chat_messages`** | `id` | UUID | Primary Key |
| | `session_id` | UUID | Foreign Key -> `chat_sessions.id` |
| | `sender_type` | VARCHAR(10) | 'USER' or 'AI' |
| | `content` | TEXT | The message body |
| | `created_at` | TIMESTAMP | Default `now()` |

---

Here is the granular, operation-by-operation breakdown of user stories and tasks for Feature 1 (Local Development & Foundation Setup) and the initial database schema implementation.

These are written so they can be dropped directly into a project management tool like Jira, Trello, or Linear.

### User Story 1: Monorepo & Docker Orchestration

**"As a developer, I want a containerized monorepo environment so that I can spin up the entire application stack locally with a single command."**

* **Task 1.1:** Create the root project directory (e.g., `stock-analysis-app`).
* **Task 1.2:** Create three subdirectories at the root level: `/frontend`, `/backend`, and `/infrastructure`.
* **Task 1.3:** Create a `.env.example` file in the root directory containing placeholder variables for POSTGRES_USER, POSTGRES_PASSWORD, POSTGRES_DB, and GEMINI_API_KEY.
* **Task 1.4:** Create a local `.env` file (and add it to `.gitignore`) by copying `.env.example`.
* **Task 1.5:** Create a `docker-compose.yml` file in the root directory.
* **Task 1.6:** Define a custom Docker bridge network in `docker-compose.yml` to allow the containers to communicate.

### User Story 2: PostgreSQL Database Provisioning

**"As a developer, I want a local PostgreSQL database running in Docker so that the backend can persist user and portfolio data."**

* **Task 2.1:** Add a `db` service to `docker-compose.yml` using the official `postgres:15-alpine` image.
* **Task 2.2:** Map local port `5432` to container port `5432` in the `db` service.
* **Task 2.3:** Map a local Docker volume to `/var/lib/postgresql/data` in the `db` service to ensure data persists across container restarts.
* **Task 2.4:** Inject the `POSTGRES_*` environment variables from the `.env` file into the `db` service.

### User Story 3: FastAPI Backend Foundation & Schema Definition

**"As a developer, I want a containerized FastAPI backend with hot-reloading and my database schemas defined so that I can begin building API routes."**

* **Task 3.1:** Create a `requirements.txt` in the `/backend` directory containing `fastapi`, `uvicorn`, `sqlalchemy`, `alembic`, `psycopg2-binary`, `python-dotenv`, and `pydantic`.
* **Task 3.2:** Create a `main.py` file in `/backend` and initialize a basic FastAPI app instance.
* **Task 3.3:** Create a `/backend/models.py` file and initialize the SQLAlchemy declarative base.
* **Task 3.4:** Define the `User` SQLAlchemy model with `id`, `email`, `password_hash`, and `created_at` columns.
* **Task 3.5:** Define the `Portfolio` SQLAlchemy model with a foreign key to `users.id` and the `account_type` column to handle categories like 'DOMESTIC' or 'EMPLOYEE_EQUITY'.
* **Task 3.6:** Define the `PortfolioItem` SQLAlchemy model with a foreign key to `portfolios.id`.
* **Task 3.7:** Define the `AnalysisHistory` SQLAlchemy model to cache Gemini's structured ratings and markdown theses.
* **Task 3.8:** Define the `ChatSession` and `ChatMessage` SQLAlchemy models to handle WebSocket memory.
* **Task 3.9:** Initialize Alembic in the `/backend` directory by running `alembic init alembic`.
* **Task 3.10:** Update `alembic.ini` and `env.py` to construct the database URL from environment variables and target the SQLAlchemy Base metadata.
* **Task 3.11:** Generate the first Alembic migration script (`alembic revision --autogenerate -m "Initial schema"`).
* **Task 3.12:** Create a `Dockerfile.dev` in the `/backend` directory using a Python 3.11 slim image, setting up the working directory, and installing requirements.
* **Task 3.13:** Set the CMD in the backend `Dockerfile.dev` to run Uvicorn with the `--reload` flag.
* **Task 3.14:** Add the `backend` service to `docker-compose.yml`, mapping the local `/backend` folder to the container to enable hot-reloading, and set it to `depends_on` the `db` service.

### User Story 4: React Frontend Foundation

**"As a developer, I want a containerized React frontend with Vite, Tailwind, and shadcn/ui configured so that I can start building the UI components."**

* **Task 4.1:** Run the Vite CLI to scaffold a React + TypeScript project inside the `/frontend` directory.
* **Task 4.2:** Install Tailwind CSS, PostCSS, and Autoprefixer via npm in the `/frontend` directory.
* **Task 4.3:** Initialize the Tailwind configuration (`npx tailwindcss init -p`).
* **Task 4.4:** Update `tailwind.config.js` to scan all React component files for utility classes.
* **Task 4.5:** Initialize `shadcn/ui` in the `/frontend` directory using their CLI tool.
* **Task 4.6:** Modify `vite.config.ts` to add a server proxy that routes all requests starting with `/api` to `http://backend:8000`.
* **Task 4.7:** Create a `Dockerfile.dev` in the `/frontend` directory using a Node.js alpine image.
* **Task 4.8:** Set the CMD in the frontend `Dockerfile.dev` to `npm run dev -- --host 0.0.0.0`.
* **Task 4.9:** Add the `frontend` service to `docker-compose.yml`, mapping the local `/frontend` folder to the container to enable Vite's Hot Module Replacement (HMR).

---

With these tasks, you have a rigid, step-by-step checklist to get the base of the app completely functional.

Would you like me to write out the actual code for the `docker-compose.yml` file and the backend `Dockerfile.dev` so you can knock out User Story 1 right away?

## Prompt 2

Let's work towards
Feature 1: Local Development & Foundation Setup

### User Story 1: Monorepo & Docker Orchestration

**"As a developer, I want a containerized monorepo environment so that I can spin up the entire application stack locally with a single command."**

* **Task 1.3:** Create a `.env.example` file in the root directory containing placeholder variables for POSTGRES_USER, POSTGRES_PASSWORD, POSTGRES_DB, and GEMINI_API_KEY.

Can you create this file?

Here is the `.env.example` file to serve as the template for your environment variables.

You should place this at the root of your monorepo. When you or another developer clone the project, you will copy this file, rename the copy to `.env`, and replace the placeholder values with your actual local credentials and API keys.

```env
# ==========================================
# PostgreSQL Database Configuration
# ==========================================
# These credentials will be used by the Docker container to initialize
# the local database and by FastAPI to connect to it.
POSTGRES_USER=local_admin
POSTGRES_PASSWORD=your_secure_local_password
POSTGRES_DB=equity_analysis_db

# ==========================================
# AI Agent Layer
# ==========================================
# Obtain your Gemini API key from Google AI Studio (https://aistudio.google.com/)
GEMINI_API_KEY=your_gemini_api_key_here

# ==========================================
# Application Settings (Optional but recommended)
# ==========================================
# Used by the backend to set CORS origins and by the frontend to proxy requests
BACKEND_PORT=8000
FRONTEND_PORT=5173

```

Would you like me to move on to **Task 1.5** and **Task 1.6** next and write the root `docker-compose.yml` file to network these services together?

## Prompt 3

Can you go ahead with task 1.6:

* **Task 1.6:** Define a custom Docker bridge network in `docker-compose.yml` to allow the containers to communicate.

Here is the complete `docker-compose.yml` file. This fulfills both Task 1.5 and Task 1.6 by defining the three core services and explicitly connecting them via a custom bridge network.

This setup ensures your containers can securely resolve each other by their service names (e.g., the backend can reach the database at `db:5432`).

Create a file named `docker-compose.yml` at the root of your monorepo and add the following configuration:

```yaml

services:
  # ==========================================
  # Database Service (PostgreSQL)
  # ==========================================
  db:
    image: postgres:18.3-alpine3.23
    container_name: stock_db
    restart: always
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - app_network

  # ==========================================
  # Backend Service (FastAPI)
  # ==========================================
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile.dev
    container_name: stock_backend
    restart: unless-stopped
    ports:
      - "${BACKEND_PORT:-8000}:8000"
    environment:
      - DATABASE_URL=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@db:5432/${POSTGRES_DB}
      - GEMINI_API_KEY=${GEMINI_API_KEY}
    volumes:
      # Mount local backend directory to /app in container for hot-reloading
      - ./backend:/app
    depends_on:
      - db
    networks:
      - app_network

  # ==========================================
  # Frontend Service (React + Vite)
  # ==========================================
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile.dev
    container_name: stock_frontend
    restart: unless-stopped
    ports:
      - "${FRONTEND_PORT:-5173}:5173"
    volumes:
      # Mount local frontend directory to /app for HMR (Hot Module Replacement)
      - ./frontend:/app
      # Prevent local node_modules from overwriting container's node_modules
      - /app/node_modules
    environment:
      - VITE_API_URL=http://localhost:${BACKEND_PORT:-8000}
    depends_on:
      - backend
    networks:
      - app_network

# ==========================================
# Task 1.6: Custom Docker Bridge Network
# ==========================================
networks:
  app_network:
    driver: bridge
    name: stock_app_network

# ==========================================
# Persistent Volumes
# ==========================================
volumes:
  postgres_data:

```

### How This Achieves Your Goal

* **Networking (`app_network`):** By defining `app_network` as a `bridge` network at the bottom and attaching all three services to it, Docker's internal DNS allows the containers to discover each other. Notice how the backend's `DATABASE_URL` uses `@db:5432` instead of `localhost`.
* **Hot-Reloading (`volumes`):** The `./backend:/app` and `./frontend:/app` volume mounts map your local code directly into the running containers. When you save a file on your host machine, Uvicorn and Vite will instantly detect the change and restart/reload.
* **Environment Injection:** Compose automatically reads the `.env` file in the same directory and passes variables like `POSTGRES_USER` directly into the PostgreSQL and FastAPI containers.

---

Would you like me to write the `Dockerfile.dev` for the FastAPI backend (Tasks 3.12 & 3.13) so the `backend` service in this compose file can successfully build?

## Prompt 4

Can you create two versions of this docker-compose.yml, one for local development and the other for production?

Here are the two distinct versions of your Docker Compose files.

It is an excellent practice to maintain both. Even though your final Production environment will be orchestrated by Terraform on Azure Container Apps, having a `docker-compose.prod.yml` allows you to test your highly optimized, compiled production containers locally before you ever push them to the cloud.

### 1. Local Development (`docker-compose.yml`)

This is optimized for the **developer experience**. It uses your `Dockerfile.dev` files, runs hot-reloading servers (Vite and Uvicorn), and uses volume mounts so that every time you save a file on your computer, the changes instantly reflect in the running container.

```yaml
services:
  db:
    image: postgres:17-alpine
    container_name: stock_db_dev
    restart: always
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    ports:
      - "5432:5432"
    volumes:
      - postgres_data_dev:/var/lib/postgresql/data
    networks:
      - app_network

  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile.dev
    container_name: stock_backend_dev
    restart: unless-stopped
    ports:
      - "${BACKEND_PORT:-8000}:8000"
    environment:
      - DATABASE_URL=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@db:5432/${POSTGRES_DB}
      - GEMINI_API_KEY=${GEMINI_API_KEY}
    volumes:
      # LIVE RELOAD: Mounts your local code into the container
      - ./backend:/app
    depends_on:
      - db
    networks:
      - app_network

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile.dev
    container_name: stock_frontend_dev
    restart: unless-stopped
    ports:
      - "${FRONTEND_PORT:-5173}:5173"
    environment:
      - VITE_API_URL=http://localhost:${BACKEND_PORT:-8000}
    volumes:
      # LIVE RELOAD: Mounts your local React code
      - ./frontend:/app
      - /app/node_modules
    depends_on:
      - backend
    networks:
      - app_network

networks:
  app_network:
    driver: bridge

volumes:
  postgres_data_dev:

```

---

### 2. Production Build Test (`docker-compose.prod.yml`)

This is optimized for **performance and security**. It builds using your `Dockerfile.prod` files. Notice that the `volumes` mapping your local code are **gone**. Instead, the code is permanently copied and compiled ("baked") into the images. The frontend is served by a high-performance Nginx web server rather than the Vite development server.

```yaml
services:
  db:
    image: postgres:17-alpine
    container_name: stock_db_prod
    restart: always
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    volumes:
      # Uses a separate volume to avoid corrupting dev data
      - postgres_data_prod:/var/lib/postgresql/data
    networks:
      - app_network

  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile.prod
    container_name: stock_backend_prod
    restart: always
    environment:
      - DATABASE_URL=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@db:5432/${POSTGRES_DB}
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      # Set to production mode
      - ENVIRONMENT=production
    ports:
      - "8000:8000"
    depends_on:
      - db
    networks:
      - app_network

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile.prod
    container_name: stock_frontend_prod
    restart: always
    ports:
      # Nginx typically serves on port 80
      - "80:80"
    depends_on:
      - backend
    networks:
      - app_network

networks:
  app_network:
    driver: bridge

volumes:
  postgres_data_prod:

```

### Key Differences to Note:

1. **No Live Code Volumes in Prod:** In `docker-compose.prod.yml`, we removed the `./backend:/app` and `./frontend:/app` volumes. Production containers must be immutable. If you change a line of code, you must rebuild the image.
2. **Separate Database Volumes:** I named the volumes `postgres_data_dev` and `postgres_data_prod`. This ensures that if you spin up the production test locally, it doesn't accidentally overwrite or conflict with the dummy data you are using for daily development.
3. **Frontend Port:** The production frontend exposes port `80` instead of `5173`, mimicking how standard web traffic will hit your Nginx server.

To run the production version locally to test your finalized build, you would use this command:
`docker compose -f docker-compose.prod.yml up --build`

---

To make both of these files work, you will need the actual Dockerfiles. Would you like me to write the `Dockerfile.dev` and `Dockerfile.prod` for your FastAPI backend next?

## Prompt 5

Can you make the docker-compse prod yaml to have the reverse proxy method? And make the corresponding change to dev docker compose yml if required

This is the perfect architectural choice. Moving to a reverse proxy setup drastically improves security by keeping your backend entirely inside the private Docker network, and it completely eliminates CORS (Cross-Origin Resource Sharing) configuration headaches.

Here is how we adjust both environments.

### 1. Production Build Test (`docker-compose.prod.yml`)

In this file, the backend port is completely removed. Nginx is the only service exposed to the host machine.

```yaml
services:
  db:
    image: postgres:17-alpine
    container_name: stock_db_prod
    restart: always
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    volumes:
      - postgres_data_prod:/var/lib/postgresql/data
    networks:
      - app_network

  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile.prod
    container_name: stock_backend_prod
    restart: always
    environment:
      - DATABASE_URL=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@db:5432/${POSTGRES_DB}
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      - ENVIRONMENT=production
    # REMOVED: ports mapping. The backend is now fully private.
    depends_on:
      - db
    networks:
      - app_network

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile.prod
    container_name: stock_frontend_prod
    restart: always
    ports:
      # ONLY the frontend is exposed to the outside world
      - "80:80"
    depends_on:
      - backend
    networks:
      - app_network

networks:
  app_network:
    driver: bridge

volumes:
  postgres_data_prod:

```

### 2. Local Development (`docker-compose.yml`)

For local development, we actually use **Vite's built-in development server** as our reverse proxy instead of Nginx. We configure Vite (via `vite.config.ts`) to intercept any request starting with `/api` and forward it to `http://backend:8000` inside the Docker network.

Because of this, your React app does *not* need the backend port exposed to the host. However, I highly recommend leaving `8000:8000` open in your dev environment for a different reason: **Developer Convenience**. Leaving it open allows you to easily access FastAPI's built-in Swagger documentation (`http://localhost:8000/docs`) and hit the API directly with tools like Postman while you write code.

Here is the updated dev file:

```yaml
services:
  db:
    image: postgres:17-alpine
    container_name: stock_db_dev
    restart: always
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    ports:
      - "5432:5432"
    volumes:
      - postgres_data_dev:/var/lib/postgresql/data
    networks:
      - app_network

  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile.dev
    container_name: stock_backend_dev
    restart: unless-stopped
    ports:
      # KEPT FOR DEV ONLY: Allows you to view Swagger UI at localhost:8000/docs
      - "${BACKEND_PORT:-8000}:8000"
    environment:
      - DATABASE_URL=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@db:5432/${POSTGRES_DB}
      - GEMINI_API_KEY=${GEMINI_API_KEY}
    volumes:
      - ./backend:/app
    depends_on:
      - db
    networks:
      - app_network

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile.dev
    container_name: stock_frontend_dev
    restart: unless-stopped
    ports:
      - "${FRONTEND_PORT:-5173}:5173"
    environment:
      # React will call itself (localhost:5173) and Vite will proxy to the backend
      - VITE_API_URL=/
    volumes:
      - ./frontend:/app
      - /app/node_modules
    depends_on:
      - backend
    networks:
      - app_network

networks:
  app_network:
    driver: bridge

volumes:
  postgres_data_dev:

```

---

### The Missing Piece: Nginx Configuration

Because your production frontend container is now responsible for routing API traffic to the backend, it needs an `nginx.conf` file to know how to do that.

Would you like me to write the `nginx.conf` routing file and the `Dockerfile.prod` for the frontend to package it all together?

## Prompt 6

Can you go ahead and write the nginx.conf routing file and the Dockerfile.prod, Dockerfile.dev

Here are the necessary Dockerfiles for both your frontend and backend, along with the Nginx configuration file.

This completes the containerization setup for both your local development workflow and your production build.

### 1. Frontend Files (React + Vite)

Place these three files inside your `/frontend` directory.

**`frontend/nginx.conf`**
This file tells the production Nginx server how to serve your React app, handle client-side routing, forward API requests, and upgrade the connection for your real-time Gemini chat WebSockets.

```nginx
server {
    listen 80;
    server_name localhost;

    # Serve the React application
    location / {
        root /usr/share/nginx/html;
        index index.html index.htm;
        # Required for React Router to handle client-side navigation
        try_files $uri $uri/ /index.html;
    }

    # Reverse Proxy for REST API requests
    location /api/ {
        proxy_pass http://backend:8000/api/;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }

    # Reverse Proxy for WebSocket connections (Feature 6)
    location /ws/ {
        proxy_pass http://backend:8000/ws/;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_read_timeout 86400; # Keep WebSocket alive
    }
}

```

**`frontend/Dockerfile.dev`**
Optimized for local development with Hot Module Replacement (HMR).

```dockerfile
FROM node:20-alpine

WORKDIR /app

# Copy package files and install dependencies
COPY package*.json ./
RUN npm install

# The rest of the code is mounted via docker-compose volumes,
# so we do not need to COPY it here.

# Start the Vite development server, binding to all network interfaces
CMD ["npm", "run", "dev", "--", "--host", "0.0.0.0"]

```

**`frontend/Dockerfile.prod`**
Uses a multi-stage build. It first compiles the React code into static files, then discards the heavy Node.js environment and places the static files into a lightweight Nginx web server.

```dockerfile
# Stage 1: Build the React application
FROM node:20-alpine AS builder

WORKDIR /app
COPY package*.json ./
RUN npm install

# Copy all source code and build
COPY . .
RUN npm run build

# Stage 2: Serve with Nginx
FROM nginx:alpine

# Remove default Nginx static assets
RUN rm -rf /usr/share/nginx/html/*

# Copy static assets from builder stage
COPY --from=builder /app/dist /usr/share/nginx/html

# Copy our custom reverse proxy configuration
COPY nginx.conf /etc/nginx/conf.d/default.conf

EXPOSE 80

CMD ["nginx", "-g", "daemon off;"]

```

---

### 2. Backend Files (FastAPI + Python)

Place these two files inside your `/backend` directory.

**`backend/Dockerfile.dev`**
Optimized for development, running Uvicorn with the `--reload` flag so your server updates instantly when you save Python files.

```dockerfile
FROM python:3.12-slim

WORKDIR /app

# Install system dependencies required for psycopg2 or Playwright
RUN apt-get update && apt-get install -y \
    build-essential \
    libpq-dev \
    && rm -rf /var/lib/apt/lists/*

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Playwright installation for the news scraper tool
RUN playwright install chromium
RUN playwright install-deps

# Start Uvicorn with hot-reloading
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--reload"]

```

**`backend/Dockerfile.prod`**
Optimized for production, copying all code into the container and dropping the reload flag for better performance.

```dockerfile
FROM python:3.12-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    build-essential \
    libpq-dev \
    && rm -rf /var/lib/apt/lists/*

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Playwright installation
RUN playwright install chromium
RUN playwright install-deps

# Copy the actual application code
COPY . .

# Expose the internal port (Nginx will proxy to this)
EXPOSE 8000

# Start Uvicorn without hot-reloading
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]

```

---

You now have the complete foundational infrastructure to build, run, and test your application locally.

Would you like me to map out the foundational code for the `/backend/main.py` and the database connection logic using SQLAlchemy, or would you prefer to start generating the React components for the frontend?


## Prompt 7

Can you reduce the proxy_read_timeout to 10 mins, we will need to implement "Heartbeats" (Pings). Have the server or client send a small "ping" message every 30–50 seconds instead.

This is an incredibly smart architectural adjustment. Relying on an infinite Nginx timeout (`86400s`) is a well-known anti-pattern because if a client silently drops off the network (like a mobile phone going through a tunnel), the server has no idea and keeps the ghost connection alive, leaking memory.

Implementing an application-level "Heartbeat" (Ping/Pong) ensures dead connections are pruned while active ones stay alive.

Here is how we update the Nginx configuration and implement the heartbeat mechanism across your frontend and backend.

### 1. Updated `nginx.conf` (Frontend)

We will reduce the `proxy_read_timeout` to 10 minutes (600 seconds). If Nginx doesn't see any data pass through the socket for 10 minutes, it will automatically cleanly sever the connection.

```nginx
    # Reverse Proxy for WebSocket connections (Feature 6)
    location /ws/ {
        proxy_pass http://backend:8000/ws/;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;

        # Reduced to 10 minutes. Requires application-level heartbeats.
        proxy_read_timeout 600s;
    }

```

### 2. The Client-Side Heartbeat (React / Zustand)

The easiest and most reliable way to handle this is to make the frontend the "pinger". When the WebSocket opens, we start a Javascript interval that sends a tiny JSON payload every 30 seconds. When the socket closes, we clear the interval.

Here is how you inject that into your `chatStore.ts`:

```typescript
let pingInterval: NodeJS.Timeout;

const connectWebSocket = (sessionId: string) => {
  const socket = new WebSocket(`${import.meta.env.VITE_WS_URL}/ws/chat/${sessionId}`);

  socket.onopen = () => {
    console.log("WebSocket Connected");

    // START THE HEARTBEAT
    // Sends a ping every 30 seconds (30000 ms) to keep Nginx routing alive
    pingInterval = setInterval(() => {
      if (socket.readyState === WebSocket.OPEN) {
        socket.send(JSON.stringify({ type: "ping" }));
      }
    }, 30000);
  };

  socket.onmessage = (event) => {
    const data = JSON.parse(event.data);

    // Ignore pong responses; they just keep the connection alive
    if (data.type === "pong") return;

    // ... handle ai_chunk and ai_complete logic here ...
  };

  socket.onclose = () => {
    console.log("WebSocket Disconnected");
    // STOP THE HEARTBEAT to prevent memory leaks
    clearInterval(pingInterval);
  };
};

```

### 3. The Server-Side Response (FastAPI)

Now that the frontend is pinging, your backend `while True:` loop needs to catch that specific message type and reply immediately so that data flows through Nginx, resetting the 10-minute timeout clock.

Here is how you update your FastAPI route logic:

```python
@router.websocket("/ws/chat/{session_id}")
async def websocket_chat_endpoint(websocket: WebSocket, session_id: str):
    await manager.connect(websocket, session_id)
    try:
        while True:
            # Wait for incoming messages from React
            data = await websocket.receive_json()

            # HANDLE HEARTBEAT
            if data.get("type") == "ping":
                await websocket.send_json({"type": "pong"})
                continue # Skip the rest of the loop, do not trigger Gemini

            # HANDLE ACTUAL CHAT MESSAGES
            if data.get("type") == "user_message":
                user_text = data.get("content")
                # ... trigger LangGraph/Gemini streaming workflow here ...

    except WebSocketDisconnect:
        manager.disconnect(session_id)

```

By doing this, data is guaranteed to pass back and forth across the Nginx proxy every 30 seconds, preventing the `600s` timeout from ever triggering while the user is actively viewing the page.

Would you like to move forward and map out the complete backend `main.py` file to get the FastAPI server initialized, or should we draft the full `websocket_manager.py` file now that the ping/pong logic is defined?

## Prompt 8

where will I add dockerignore?

You will need to create **two separate `.dockerignore` files**, placing one inside the `/frontend` directory and one inside the `/backend` directory.

Because your `docker-compose.yml` file specifies different build contexts for each service (`context: ./frontend` and `context: ./backend`), the Docker engine looks for the `.dockerignore` file at the root of each specific context, not at the root of your entire monorepo.

This is critical for performance. Without these files, Docker will attempt to copy your massive local `node_modules` or Python virtual environments to the Docker daemon before building the images, which takes forever and inflates your image size.

Here is exactly what should go in each file.

### 1. Frontend `.dockerignore`

Create a file named `.dockerignore` inside the `/frontend` directory.

```text
# Dependency directories
node_modules/
npm-debug.log

# Build outputs
dist/
build/

# Environment variables (Keep secrets out of the image)
.env
.env.local
.env.*

# Git and OS files
.git/
.DS_Store
Thumbs.db

# Docker-related files (no need to copy the Dockerfiles into the image)
Dockerfile.dev
Dockerfile.prod

```

### 2. Backend `.dockerignore`

Create a file named `.dockerignore` inside the `/backend` directory.

```text
# Virtual environments
venv/
.venv/
env/

# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# Environment variables
.env

# Git and OS files
.git/
.DS_Store
Thumbs.db

# Database files (if you accidentally use SQLite locally instead of Postgres)
*.sqlite3

# Docker-related files
Dockerfile.dev
Dockerfile.prod

```

### Why this matters for your specific setup:

In your `Dockerfile.prod` for the frontend, you have the line `COPY . .`. If `node_modules/` wasn't in your `.dockerignore`, Docker would copy your entire local `node_modules` folder into the container, overwriting the clean Linux dependencies installed by `RUN npm install` just a step prior, which often breaks the build entirely.

---

With your Docker orchestration, reverse proxies, and ignore files fully established, your infrastructure foundation is rock solid.

Would you like to move into **User Story 3** and start writing the `/backend/main.py` file to initialize the FastAPI app and connect it to your PostgreSQL database?

